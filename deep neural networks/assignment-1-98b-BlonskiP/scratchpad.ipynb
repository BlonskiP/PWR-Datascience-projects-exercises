{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import abc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "# Wczytanie zbioru danych\n",
    "dataset_path = 'dataset.pkl'\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    dataset = pickle.load(f)  \n",
    "\n",
    "images_raw_train = dataset['train']['X']\n",
    "images_raw_test = dataset['test']['X']\n",
    "images_raw_valid = dataset['validation']['X']\n",
    "\n",
    "labels = dataset['train']['y']\n",
    "labels_test = dataset['test']['y']\n",
    "lebels_valid = dataset['validation']['y']\n",
    "# Ustawienie ziarna\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "# Zmiana rozmiaru obrazu z 224x224 do 64x64\n",
    "images = tf.image.resize(\n",
    "    images_raw_train, [64, 64], \n",
    "    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR, \n",
    "    preserve_aspect_ratio=False,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "images_test = tf.image.resize(\n",
    "    images_raw_test, [64, 64], \n",
    "    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR, \n",
    "    preserve_aspect_ratio=False,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "images_valid = tf.image.resize(\n",
    "    images_raw_valid, [64, 64], \n",
    "    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR, \n",
    "    preserve_aspect_ratio=False,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "# Zmiana typu obrazów z int na float i normalizacja wartości pikseli z przedziału 0-255 do przedziału 0-1\n",
    "images = tf.cast(images, tf.float32) / 255.0\n",
    "images_test = tf.cast(images_test, tf.float32) / 255.0\n",
    "images_valid = tf.cast(images_valid, tf.float32) / 255.0\n",
    "# Zmiana wymiarowości z [8870] na [8870, 1] w celu zastosowania `SparseCategoricalCrossentropy`\n",
    "labels = tf.reshape(labels, [-1, 1])\n",
    "labels_test = tf.reshape(labels_test, [-1, 1])\n",
    "lebels_valid = tf.reshape(lebels_valid, [-1, 1])\n",
    "# Do iterowania zbioru danych i podzielenia na paczki (ang. batch) można wykorzystać interfejs zdefiniowany w `tensorflow.data.Dataset`\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((images, labels)) \n",
    "train_ds = train_ds.shuffle(buffer_size=len(images)).batch(batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "\n",
    "\n",
    "class AbstractLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Abstract Layer.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Inits the class.\"\"\"\n",
    "        super(AbstractLayer, self).__init__()\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Makes forward pass of the layer.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class AbstractModel(tf.keras.Model):\n",
    "    \"\"\"Abstract model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Inits the class.\"\"\"\n",
    "        super(AbstractModel, self).__init__()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Makes forward pass of the network.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, **kwargs):\n",
    "        \"\"\"Fits the model.\n",
    "\n",
    "        Replaces original fit method.\n",
    "        \n",
    "        More information can be found in documentation:\n",
    "        https://www.tensorflow.org/guide/eager\n",
    "        https://www.tensorflow.org/guide/autodiff\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts x\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import tensorflow as tf\n",
    "\n",
    "class MLP_Layer(AbstractLayer):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        \"\"\"Inits the class.\"\"\"\n",
    "        self.units = units\n",
    "        super(MLP_Layer, self).__init__()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Here we build our layer. Here will be create Weights and baiases\"\"\"\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                      initializer=tf.random_uniform_initializer,\n",
    "                                      trainable=True)\n",
    "        \n",
    "        self.b = self.add_weight(shape=(self.units,),initializer='random_normal',trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Makes forward pass of the layer.\"\"\"\n",
    "        #y = a*x +b  -> weights * input + bias\n",
    "        ax = tf.matmul(inputs, self.w)\n",
    "        y = ax + self.b\n",
    "        return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "class MLP(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        self.loss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=True) #for one hot encoding\n",
    "        self.full_connected_1 = MLP_Layer(units=256)\n",
    "        #full connected 14 units Softmax\n",
    "        self.full_connected_2 = MLP_Layer(units=14)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Makes forward pass of the network.\"\"\"\n",
    "        x = tf.reshape(inputs,[inputs.shape[0],64*64*3])\n",
    "        x = self.full_connected_1(x)\n",
    "        x = tf.nn.relu(x) #Activation 1 layer\n",
    "        x = self.full_connected_2(x)\n",
    "        x = tf.nn.softmax(x) #Activation 2 layer\n",
    "        x=tf.keras.backend.sum(loss)\n",
    "        return x\n",
    "\n",
    "    def fit(self, **kwargs):\n",
    "        history = {'loss':[],\n",
    "                   'accuracy':[],\n",
    "                   'Fscore':[]}\n",
    "        batch_size = 100\n",
    "        #print(type(history['loss']))\n",
    "        train_dataset= kwargs['dataset']\n",
    "        test_dataset= kwargs['test_ds']\n",
    "        valid_dataset = kwargs['valid_ds']\n",
    "        num_epochs = kwargs['epochs']\n",
    "        loss_list = []\n",
    "        NUMBER_OF_CLASSES = 14\n",
    "        prec = tf.keras.metrics.Precision()\n",
    "        recall = tf.keras.metrics.Recall()\n",
    "        acc = tf.keras.metrics.Accuracy()\n",
    "        if(train_dataset!=None):\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                acc.reset_states() #reset acc\n",
    "                prec.reset_states()\n",
    "                recall.reset_states()\n",
    "                \n",
    "                train_ds = tf.data.Dataset.from_tensor_slices((images, labels)) \n",
    "                size=len(train_dataset)\n",
    "                train_ds = train_ds.shuffle(buffer_size=size).batch(batch_size=batch_size)\n",
    "                \n",
    "                loss_sum = 0\n",
    "                for x, y in train_ds:\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        y_onehot = tf.reshape(tf.one_hot(y,depth=NUMBER_OF_CLASSES),(len(y),NUMBER_OF_CLASSES))\n",
    "                        y_prediction = self.call(x,training=True)\n",
    "                        \n",
    "                        acc.update_state(y_pred=y_prediction,y_true=y_onehot)\n",
    "                        recall.update_state(y_pred=y_prediction,y_true=y_onehot)\n",
    "                        prec.update_state(y_pred=y_prediction,y_true=y_onehot)\n",
    "                        loss = self.loss_fn(y_onehot,y_prediction)\n",
    "                        loss_sum+=tf.keras.backend.sum(loss)\n",
    "                        \n",
    "                    grads = tape.gradient(loss, self.trainable_variables)\n",
    "                    self.optim.apply_gradients(zip(grads,self.trainable_variables))\n",
    "                r = recall.result().numpy()\n",
    "                p = prec.result().numpy()\n",
    "                F1 = 2*p*r\n",
    "                F1 = F1/(p+r)\n",
    "                history['accuracy'].append(m.result().numpy())\n",
    "                history['loss'].append((loss_sum/size).numpy())\n",
    "                history['Fscore'].append(F1)\n",
    "        else:\n",
    "            print('provide dataset')\n",
    "              \n",
    "        return history\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts x\"\"\"\n",
    "        pred = self(x,training=False)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images_test_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f7c1a1f04aae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages_test_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'images_test_ds' is not defined"
     ]
    }
   ],
   "source": [
    "model=MLP()\n",
    "history = model.fit(dataset=images,epochs=25,test=images_test_ds,valid=images_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "Cacc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "acc = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[0,0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.constant([[1,0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cacc.update_state(y_true=y_true,y_pred=y_pred)\n",
    "acc.update_state(y_true=y_true,y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "print(Cacc.result().numpy())\n",
    "print(acc.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cacc.reset_states()\n",
    "acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[0,0,0,0,1]])\n",
    "y_pred = tf.constant([[1,0,0,0,1]])\n",
    "recall = tf.keras.metrics.Recall()\n",
    "recall.update_state(y_true=y_true,y_pred=y_pred)\n",
    "res = recall.result()\n",
    "recall.reset_states()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
