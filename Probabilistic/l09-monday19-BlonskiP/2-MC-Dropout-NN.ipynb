{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout as a Bayesian Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already introduced a few concepts that are popular among the neural networks community. These include standard neural networks and their bayesian formulation. However, training bayesian neural nets can be daunting: you have to implement initialization carefully and store twice as many parameters. It also requires retraining (so no knowledge transfer between neural networks can happen), and the learning process has high variance. \n",
    "\n",
    "To tackle these problems [Yarin Gal and Zoubin Ghahramani in \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\"](https://arxiv.org/abs/1506.02142) asked the following question: is it possible to approximate the bayesian behavior using existing well-established techniques? The answer is yes, and you will find out the solution in that notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout as an approximation of Gaussian Processes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already shown you how the dropout technique works. It bases on sampling mask from the bernoulli mask with a probability $1 - p$ as $M \\sim B(1 - p)$ and multiply layers inputs by that mask as $\\hat{x} = x \\odot M$, where $\\odot$ is a hadamard product (elementwise multiplication). This operation is performed during training and turned off during validation. What if we left the dropout during the validation?\n",
    "\n",
    "It turns out that we get other architecture of the same neural network. Thanks to the possibility of multiple samplings, we get a Gaussian Process with a covariance function marginalized over its weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For recall, the predictive distribution for a pair $(x, y)$ of test point and weights $\\theta$ is equal to:\n",
    "\n",
    "$$\n",
    "q(y | x) = \\int p(y | x; \\theta) q(\\theta)\\, d\\theta\n",
    "$$\n",
    "\n",
    "The equation can be understood as the integration over multiple instances of $\\theta$, where multiple of these instances are obtained by multiple forward passes with dropout turned on. We have to find the first two raw moments (mean and standard deviation) to obtain the full uncertainty. These can be easily approximated through Monte Carlo sampling with $T$ samplings of bernoulli mask as:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{q(y|x)} \\approx \\frac{1}{T} \\sum^T_{t=1} \\hat{f}(x; \\theta^t)\n",
    "$$\n",
    "\n",
    "where $\\hat{f}$ is our neural network. Predictive variance however is equal to:\n",
    "\n",
    "$$\n",
    "\\text{Var}_{q(y|x)}(y) \\approx \\tau^{-1} + \\frac{1}{T}\\sum^T_{t=1}\\hat{f}(x; \\theta^t)^2 - \\mathbb{E}_{q(y|x)}^2 \n",
    "$$\n",
    "\n",
    "Note that these equations are simplified versions for the case when only a single scalar is predicted. Fortunately, if no dependencies occur between classes, then each predicted scalar in a vector can be treated independently (ex. for categorical prediction). For full derivation, please refer to the original work. \n",
    "\n",
    "Taking advantage of the central limit theorem, we can see that these equations lead to a description of normal distributions, where in fact, each forward inference uses bernoulli distribution. This where $\\tau$ appears - square root of its inverse is our width of the normal distribution of multiple bernoulli trials. Recall that $\\tau$ is an inverse of the variance. It is described as:\n",
    "\n",
    "$$\n",
    "\\tau = \\frac{(1 - p)l^2}{2N\\lambda}\n",
    "$$\n",
    "\n",
    "where $p$ is a probability of the dropout, $N$ is the number of samples in the training data, $\\lambda$ - $L_2$ strength, and $l$ - a prior scale of the distribution. Notice that with the increase of data, width decreases since we are given more data, and we are more sure about the predicted values. \n",
    "\n",
    "\n",
    "Note, that both moments simplify to normal mean and variance calculation of predictions.\n",
    "\n",
    "Finally, we can obtain log-likelihood of the model as:\n",
    "\n",
    "$$\n",
    "\\log p(y | x; \\theta) \\approx \\text{logsumexp}\\left(- \\frac{1}{2} \\tau (y - y^t)^2 \\right) - \\log T - \\frac{1}{2}\\log 2\\pi - \\frac{1}{2} \\log \\tau^{-1}\n",
    "$$\n",
    "\n",
    "$\\text{logsumexp}$ trick is a mathematical equation, used in summing probabilites for a numerical stability. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{logsumexp}\\left(- \\frac{1}{2} \\tau (y - y^t)^2 \\right) = \\log\\left( \\sum^T_{t=1} \\exp\\left(-\\frac{1}{2}\\tau(y - y^t)^2\\right) \\right)\n",
    "$$\n",
    "\n",
    "It is implemented `pytorch` in method `torch.logsumexp(...)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Monte Carlo Dropout neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will work in the same setting as in the previous notebook. This includes the MNIST dataset and performed analysis. Your task is as follows:\n",
    "- Implement a linear layer to turns dropout, no matter whether it is training or validation (use of `self.training` variable in the `nn.Module`)\n",
    "- Implement calculating log-likelihood of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.nn import OwnSigmoid, OwnSoftmax, MSELoss\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing custom bayesian linear layer and sigmoid activation function\n",
    "\n",
    "class MCDropoutLinear(nn.Linear):   # subclassing nn.Module, definition of our own bayesian fully connected layer\n",
    "    \"\"\"Main reference: https://arxiv.org/pdf/1505.05424.pdf\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_input_features: int,  # number of input features\n",
    "        num_output_features: int,  # number of output features\n",
    "        use_bias: bool,  # whether to use bias\n",
    "        probability: float,\n",
    "    ):\n",
    "        \"\"\"Implement initialization of weights and biases values\"\"\"\n",
    "        super().__init__(num_input_features, num_output_features, bias=use_bias)\n",
    "        assert 0 <= probability <= 1\n",
    "        \n",
    "        #param init\n",
    "        self.use_bias = use_bias\n",
    "        self.num_input_features = num_input_features  # number of input features\n",
    "        self.num_output_features = num_output_features  # number of output features\n",
    "        # weights and bias\n",
    "        \n",
    "        self.weights_for_dropout = nn.Parameter(torch.Tensor(num_output_features, num_input_features).uniform_(-0.2, 0.2))\n",
    "        # handling bias\n",
    "        if self.use_bias == True:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_output_features).uniform_(-0.2, 0.2))\n",
    "        else:\n",
    "            self.bias = 0\n",
    "        #dropout prob\n",
    "        self.dropout_probability = probability\n",
    "              \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Implement forward inference using reparametrization trick\"\"\"\n",
    "        #tensor out x in of wages where should droput be used.\n",
    "        dropout_tensor = torch.Tensor(self.weights_for_dropout.size())\n",
    "        dropout_tensor = nn.Parameter(dropout_tensor.bernoulli_(self.dropout_probability))\n",
    "        #matrix multip where weights has same shape as dropout tensor\n",
    "        self.weight = nn.Parameter(self.weights_for_dropout * dropout_tensor)\n",
    "        x=x/self.dropout_probability\n",
    "        #super().forward -> nn.Linear ->  def forward(self, input): return F.linear(input, self.weight, self.bias)\n",
    "        return super().forward(x)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropoutMLP(nn.Module):  # subclassing nn.Module, definition of our MC dropout multilayer perceptron\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_input_features: int,  # number of input features (28 x 28 for MNIST)\n",
    "        num_hidden_features: int,  # number of hidden units\n",
    "        num_output_classes: int,  # number of output classes\n",
    "        hidden_activation_function: type,  #  hidden activation function class\n",
    "        output_activation_function: type,  # output activation function class\n",
    "        probability: float,  # probability of the dropout\n",
    "        train_data_size: int,  # size of the training data size, necessary for the tau parameter\n",
    "        lengthscale: float = 1e-2,  # l parameter in tau,\n",
    "        weight_decay: float = 1e-5,  # weight decay of L2\n",
    "    ):\n",
    "        \"\"\"Implement neural network, similar to one in `0-NN` but with use of Bayesian layers\"\"\"\n",
    "        super().__init__()\n",
    "        #param init\n",
    "        self.num_input_features = num_input_features #: int,  # number of input features (28 x 28 for MNIST)\n",
    "        self.num_hidden_features = num_hidden_features#: int,  # number of hidden units\n",
    "        self.num_output_classes = num_output_classes # int,  # number of output classes\n",
    "        self.hidden_activation_function = hidden_activation_function() # type,  #  hidden activation function class\n",
    "        self.output_activation_function = output_activation_function()# type,  # output activation function class\n",
    "        self.probability = probability #float,  # probability of the dropout\n",
    "        self.train_data_size = train_data_size # int,  # size of the training data size, necessary for the tau parameter\n",
    "        self.lengthscale = lengthscale #  float = 1e-2,  # l parameter in tau,\n",
    "        self.weight_decay = weight_decay #  float = 1e-5,  # weight decay of L2\n",
    "        \n",
    "        \n",
    "        # LAYERS\n",
    "        # Input to first hidden layer\n",
    "        self.layer_input_to_hidden = MCDropoutLinear(num_input_features, num_hidden_features, use_bias=True, probability=probability)\n",
    "        # Hidden layer to output\n",
    "        self.layer_hidden_to_output = MCDropoutLinear(num_hidden_features, num_output_classes, use_bias=True, probability=probability)\n",
    "        # Ï„ =pl^2  / 2N~   (~ - lamda :D) https://arxiv.org/pdf/1506.02142.pdf 4.7\n",
    "        nominator = ((1-probability) * lengthscale**2) \n",
    "        denominator = (2 * train_data_size * weight_decay)\n",
    "        self.tau = nominator/denominator \n",
    "        # denominator can't be 0 because we always have train data and weight_dacy > 0\n",
    "        \n",
    "        \n",
    "    # definition of the forward inference\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Implement forward inference for MC dropout layers (single forward pass)\"\"\"\n",
    "        x = self.hidden_activation_function(self.layer_input_to_hidden(x))\n",
    "        x = self.output_activation_function(self.layer_hidden_to_output(x))\n",
    "        return x\n",
    "        \n",
    "    def prediction_log_likelihood(self, predictions: torch.Tensor, true: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Implement calculation of model likelihood\n",
    "       \n",
    "        Predictions are in the form of N x K x T matrix, where N is a number\n",
    "        of samples, K number of classes and T number of samplings while true - N x K.\n",
    "        \"\"\"\n",
    "       \n",
    "        expected_outputs = true.clone().unsqueeze_(-1)\n",
    "        expected_outputs = expected_outputs.expand(predictions.shape)\n",
    "        \n",
    "        #https://arxiv.org/pdf/1506.02142.pdf 4.8 equasion - predictive log likelihood estimation\n",
    "        inside_log = -0.5 * self.tau * ((predictions - expected_outputs).pow(2)).sum(1).sqrt().mean(0)\n",
    "        log = torch.logsumexp(inside_log, 0)\n",
    "        log_T = math.log(self.train_data_size)\n",
    "        log_pi = 0.5 * math.log(2*math.pi)\n",
    "        log_tau = 0.5 * math.log(self.tau**(-1))\n",
    "        res = log - log_T - log_pi - log_tau\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = utils.load_mnist_datasets(limit_train_samples_to=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = len(train_dataset)\n",
    "model = MCDropoutMLP(\n",
    "    num_input_features=28 * 28,  # pixels\n",
    "    num_hidden_features=128,  # arbitrary number\n",
    "    num_output_classes=10,  # num of digits\n",
    "    hidden_activation_function=OwnSigmoid,\n",
    "    output_activation_function=OwnSoftmax,\n",
    "    train_data_size= train_data_size,\n",
    "    probability=0.4,\n",
    "    lengthscale=1e-2,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "\n",
    "loss_fun = MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=1e-3, \n",
    "    weight_decay=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b840cea7b743fabf2bb2768f7dc89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=312.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce043a738164252a1ab7a4009c4df06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=312.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85aeb3c28dd5453a85b7611209909f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=312.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9a57127dda4309bd57859f93f66340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=312.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930f36f7f13c4d8bb9351775f4b16dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=312.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 401408 bytes. Buy new RAM!\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-418be0d5558c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train_metrics, test_metrics = utils.fit_mc_dropout(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalid_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mloss_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Studia\\PUMA\\l09-monday19-BlonskiP\\src\\utils.py\u001b[0m in \u001b[0;36mfit_mc_dropout\u001b[1;34m(model, train_dataset, valid_dataset, loss_function, batch_size, epochs, optimizer, num_samplings)\u001b[0m\n\u001b[0;32m    432\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_samplings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# applying gradients (partial derivatives)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\lab\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\lab\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 401408 bytes. Buy new RAM!\n"
     ]
    }
   ],
   "source": [
    "train_metrics, test_metrics = utils.fit_mc_dropout(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=test_dataset,\n",
    "    loss_function=loss_fun,\n",
    "    batch_size=32,\n",
    "    epochs=5, # 20 was to many -> ram error\n",
    "    optimizer=optimizer,\n",
    "    num_samplings = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.show_learning_curve(train_metrics, test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.show_accuracy_curve(train_metrics, test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.show_log_prob_curve(train_metrics, test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best training accuracy: {max(train_metrics['acc'])}\")\n",
    "print(f\"Best testing accuracy: {max(test_metrics['acc'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samplings = 10\n",
    "analyzer = utils.Analyzer(model, test_dataset, num_samplings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_samples(\n",
    "    *analyzer.get_top_k_high_confidence_correct(10)\n",
    ")\n",
    "print(\"Top high confidence correct predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_samples(\n",
    "    *analyzer.get_top_k_low_confidence_correct(10)\n",
    ")\n",
    "print(\"Top low confidence correct predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_samples(\n",
    "    *analyzer.get_top_k_low_confidence_mistakes(10)\n",
    ")\n",
    "\n",
    "print(\"Top low confidence wrong predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_samples(\n",
    "    *analyzer.get_top_k_high_confidence_mistakes(10)\n",
    ")\n",
    "\n",
    "print(\"Top high confidence wrong predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_weights(\n",
    "    #TODO: get tensor of the first layer to the hidden layer weights in your model, \n",
    "    \"layer input -> hidden\"\n",
    ")\n",
    "print(\"Histogram of weights for layer 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_weights(\n",
    "    #TODO: get tensor of the second layer to the output layer weights in your model, \n",
    "    \"layer hidden -> output\"\n",
    ")\n",
    "print(\"Histogram of weights for layer 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Parameter(torch.Tensor(5, 5).bernoulli_(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
