{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Matrix factorization\n",
    "=======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Matrix factorization is a technique used to decompose a matrix into two smaller matrices, so that the multiplication of these matrices is equal to the original matrix.\n",
    "\n",
    "One of the popular applications of this technique are recommender systems, where MF is used to extract common hidden features shared between each user (first axis of the matrix) and item to be recommended (second axis), which explain the interactions between them. Those features need to be inferred from the data. This approach is described wider in [the article](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf).\n",
    "\n",
    "![image.png](https://miro.medium.com/max/3378/0*k0m45RKFE7YN3UHf.png)\n",
    "\n",
    "The original matrix shape is $m \\times n$. The result should be two matrices:\n",
    "  *  of shape $m \\times d$\n",
    "  *  of shape $d \\times n$\n",
    "where $d$ is chosen to compromise on computation efficiency and number of features required to represent the data.\n",
    "\n",
    "The value $\\hat{r}_{ui}$ from the original matrix (originally - a matrix of ratings of items) is calculated as the dot product of a single row from the first matrix $p_u$ (vector representing a user) and a single column of the second one $q_i$ (vector representing an item).\n",
    "\n",
    "$$\\hat{r}_{ui} = q_i^T p_u$$\n",
    "\n",
    "To learn the factor vectors $\\mathbf{p}$ and $\\mathbf{q}$ one can use for example Mean Squared Error:\n",
    "\n",
    "$$MSE = \\sum_{(u, i)\\in \\mathcal{K}} \\left(r_{ui} - q_i^T p_u\\right)^2$$\n",
    "\n",
    "where $\\mathcal{K}$ is the set of $(u, i)$ pairs, for which the true $r_{ui}$ is known.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To avoid overfitting, various **regularizations** can be used, such as **L2 regularization** (*ridge regression*):\n",
    "\n",
    "$$L2 = \\sum_{j=0}^p \\beta_j^2$$\n",
    "\n",
    "The L2 term, added with coefficient $\\lambda$ to the loss function encourages the params $\\beta_j$ to be small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Homework\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Create a class inheriting from `torch.nn.Module`. Declare $\\mathbf{p}$ and $\\mathbf{q}$ matrices and set their sizes according to the passed params. Define `forward(self)` method to return a dot product of the matrices.\n",
    "2. Create a basic loss function (which will be , according to the equation:\n",
    "$$L = MSE + \\lambda \\cdot L2$$\n",
    "   You might implement your own functions or use the PyTorch builtins.\n",
    "   > *NOTE*: most PyTorch optimizers already include L2 regularization, controlled by `weight_decay` param, but we will not be using it here.\n",
    "3. Create a function `train` that will train the model for a given number of epochs, with a given learning rate. Enable passing the source matrix and the model. Train the model on matrix given at the start, with given params. Plot the loss function.\n",
    "4. Expand the loss function and add additional constraints:\n",
    "   *  apply L1 regularization\n",
    "   $$L1 = \\sum_{j=0}^p \\left|\\beta_j\\right|$$\n",
    "   *  encourage the length of each vector (i.e. per one person or item) to be 1\n",
    "   $$L_{unit} = \\sum_{u \\in \\mathcal{K}} \\left| |p_u| - 1\\right| + \\sum_{i \\in \\mathcal{K}} \\left|\\left|q_i\\right| - 1\\right|$$\n",
    "   \n",
    "   Verify their influence on the achieved loss.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Task 1\"\"\"\n",
    "\n",
    "class MatrixFactorizer(torch.nn.Module):\n",
    "    def __init__(self, dim_h, dim_w, n_features):\n",
    "        \"\"\" Initialize the model.\n",
    "        \n",
    "        :param dim_h: number of rows in \"user\" matrix\n",
    "        :param dim_w: number of columns in \"item\" matrix\n",
    "        :param n_features: number of features in both matrices\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.p = torch.from_numpy(np.random.rand(dim_h,n_features))\n",
    "        \n",
    "        self.q = torch.from_numpy(np.random.rand(dim_w,n_features))\n",
    "       \n",
    "      \n",
    "        self.q.requires_grad_(True)\n",
    "        self.p.requires_grad_(True)\n",
    "        pass\n",
    "        \n",
    "    def forward(self):\n",
    "        return torch.mm(self.p,self.q.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_mf():\n",
    "    params = {(3, 5, 1): (3, 5),\n",
    "              (7, 7, 1): (7, 7),\n",
    "              (49, 35, 3): (49, 35)}\n",
    "    for (dim_h, dim_w, n_features), (result_h, result_w) in params.items():\n",
    "        mf = MatrixFactorizer(dim_h, dim_w, n_features)\n",
    "        assert (mf().size() == (result_h, result_w))\n",
    "\n",
    "\n",
    "test_mf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Task 2\"\"\"\n",
    "def loss_fn(r, factorizer, rate=0.01):\n",
    "    \"\"\" Calculate the loss.\n",
    "    \n",
    "    :param r: the original matrix\n",
    "    :param mf: the matrix factorizer model, initialized\n",
    "    :param rate: l2 regularization term rate\n",
    "    :return: loss function value\n",
    "    \"\"\"\n",
    "    return torch.sum(((r - factorizer)**2))\n",
    "    pass\n",
    "    \n",
    "def l2_norm(params): \n",
    "    return torch.sum(params**2)\n",
    "    pass    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_l2():\n",
    "    params = {(3.,): 9.00,\n",
    "              (0.4, 0.6): 0.52,\n",
    "              ((0.1, 0.2), (0.4, 0.5)): 0.46}\n",
    "    for data, result in params.items():\n",
    "        data = torch.tensor(data, dtype=torch.float)\n",
    "        result = torch.tensor(result, dtype=torch.float)\n",
    "        l2 = torch.round(\n",
    "            100 * l2_norm(data)\n",
    "        ) / 100\n",
    "        assert l2 == result\n",
    "        \n",
    "test_l2()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Task 3\"\"\"\n",
    "def train(model, data,\n",
    "          num_epochs=10_000,\n",
    "          lr=1e-3,\n",
    "          loss_fn=loss_fn):\n",
    "    \"\"\" Train MF model.\n",
    "    \n",
    "    :param model: MatrixFactorizer model (initialized)\n",
    "    :param data: matrix to be factorized\n",
    "    :param num_epochs: number of epochs to perform\n",
    "    :param lr: used learning rate\n",
    "    :param loss: loss function (callable)\n",
    "    :return: loss history\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        predicted_matrix = model()\n",
    "        loss = loss_fn(data,model)\n",
    "        \n",
    "        loss.backward()\n",
    "        loss_history.append(loss)\n",
    "        with torch.no_grad():\n",
    "            model.p -= model.p.grad * lr\n",
    "            model.q -= model.q.grad * lr\n",
    "            model.p.grad.zero_()\n",
    "            model.q.grad.zero_()\n",
    "\n",
    "        \n",
    "    plt.plot(loss_history)   \n",
    "    plt.show()\n",
    "    return loss_history\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Task 4\"\"\"\n",
    "def l1_norm(params): #wykorzystaÄ‡ operacje macierzowe\n",
    "    l1 = 0\n",
    "    params_reshaped_tensor = torch.reshape(params, (-1,))\n",
    "    for param in params_reshaped_tensor:\n",
    "        l1 += torch.abs(param)\n",
    "   # print(l1.requires_grad)\n",
    "    return l1\n",
    "    pass\n",
    "\n",
    "def length_reg(params): #params to p i q z factorizera  \n",
    "    \n",
    "    loss = torch.norm(params,dim=-1) #\n",
    "  #  print(loss)\n",
    "    distance = torch.sum(torch.abs(1-loss))\n",
    "  #  print(distance)\n",
    "    \n",
    "    #for u in range(0,params.size(0)-1):\n",
    "    #    for i in range(0,params.size(0)):\n",
    "  #  print(distance.requires_grad)\n",
    "    return distance\n",
    "    pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_l1():\n",
    "    params = {(3.,): 3.00,\n",
    "              (0.4, 0.6): 1.0,\n",
    "              (0.1, 0.2, 0.4, 0.5): 1.2}\n",
    "    for data, result in params.items():\n",
    "        data = torch.tensor(data, dtype=torch.float)\n",
    "        result = torch.tensor(result, dtype=torch.float)\n",
    "        l1 = torch.round(\n",
    "            100 * l1_norm(data)\n",
    "        ) / 100\n",
    "        assert l1 == result\n",
    "        \n",
    "test_l1()\n",
    "\n",
    "#test_random_tensor =  torch.from_numpy(np.empty([4,4])) \n",
    "#test_Factorizer = MatrixFactorizer(4,4,1)\n",
    "#train(test_Factorizer,test_random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_length_reg():\n",
    "    params = {((3.,),): 2.00,\n",
    "              ((0.4, 0.6),): 0.28,\n",
    "              ((0.1, 0.2), (0.4, 0.5)): 1.14}\n",
    "    for data, result in params.items():\n",
    "        data = torch.tensor(data, dtype=torch.float)\n",
    "        result = torch.tensor(result, dtype=torch.float)\n",
    "        reg = torch.round(\n",
    "            100 * length_reg(data)\n",
    "        ) / 100\n",
    "        assert reg == result\n",
    "\n",
    "test_length_reg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn_reg(r, factorizer, l1_rate=0., l2_rate=0.1, length_rate = 0.2):\n",
    "    \"\"\" Calculate the loss.\n",
    "    \n",
    "    :param r: the original matrix\n",
    "    :param mf: the matrix factorizer model, initialized\n",
    "    :param l1_rate: l1 regularization term rate\n",
    "    :param l2_rate: l2 regularization term rate\n",
    "    :param length_rate: length regularization term rate\n",
    "    :return: loss function value\n",
    "    \"\"\"\n",
    "    factor = factorizer()\n",
    "    loss_value = loss_fn(r,factor)\n",
    "    l2 = l2_rate*l2_norm(factorizer.p)+l2_norm(factorizer.q)*l2_rate\n",
    "    l1 = l1_rate*l1_norm(factorizer.p)+l1_norm(factorizer.q)*l1_rate\n",
    "    length_reg_value = length_reg(factor * length_rate)\n",
    "    loss = loss_value + l2 + l1 + length_reg_value\n",
    "  #  print(loss_value.requires_grad)\n",
    "  ##  print(l1.requires_grad)\n",
    "   # print(l2.requires_grad)\n",
    "   # print(length_reg_value.requires_grad)\n",
    "   # print(loss.requires_grad)\n",
    "    return loss\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(model, data,\n",
    "          num_epochs=10_000,\n",
    "          lr=1e-3,\n",
    "          loss_fn=loss_fn):\n",
    "    \"\"\" Train MF model.\n",
    "    \n",
    "    :param model: MatrixFactorizer model (initialized)\n",
    "    :param data: matrix to be factorized\n",
    "    :param num_epochs: number of epochs to perform\n",
    "    :param lr: used learning rate\n",
    "    :param loss: loss function (callable)\n",
    "    :return: loss history\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        predicted_matrix = model()\n",
    "        loss = loss_fn_reg(data,model)\n",
    "        \n",
    "        loss.backward()\n",
    "        loss_history.append(loss)\n",
    "        with torch.no_grad():\n",
    "            model.p -= model.p.grad * lr\n",
    "            model.q -= model.q.grad * lr\n",
    "            model.p.grad.zero_()\n",
    "            model.q.grad.zero_()\n",
    "\n",
    "        \n",
    "    plt.plot(loss_history)   \n",
    "    plt.show()\n",
    "    return loss_history\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tensor(6.1583, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(6.1314, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(6.1050, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(6.0791, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(6.0537, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(6.0287, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(6.0042, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.9801, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.9564, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.9332, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.9103, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.8878, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.8658, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.8441, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.8227, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.8017, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.7811, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.7608, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.7409, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.7213, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.7020, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.6830, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.6643, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.6459, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.6278, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.6100, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.5925, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.5753, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.5583, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.5416, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.5252, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.5090, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.4931, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.4774, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.4620, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.4467, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.4318, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.4170, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.4025, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.3881, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.3740, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.3601, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.3464, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.3329, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.3196, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.3065, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.2936, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.2808, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.2683, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.2559, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.2437, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.2317, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.2198, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.2081, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.1966, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.1852, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.1740, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.1629, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.1520, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.1412, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.1306, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.1201, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.1098, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.0996, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.0895, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.0796, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.0698, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.0601, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.0506, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.0411, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.0318, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.0226, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.0136, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(5.0046, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.9958, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.9871, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.9785, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.9700, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.9616, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.9533, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.9451, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.9370, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.9290, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.9211, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.9133, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.9056, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8980, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8905, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8831, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8757, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8685, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8613, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8543, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8473, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8404, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8335, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8268, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8201, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8135, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8070, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.8005, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7942, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7879, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7817, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7755, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7694, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7634, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7575, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7516, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7458, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7400, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7343, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7287, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7231, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7176, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7122, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7068, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.7015, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6962, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6910, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6858, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6807, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6757, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6707, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6657, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6608, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6560, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6512, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6465, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6418, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6372, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6326, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6280, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6235, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6191, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6147, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6103, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6060, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.6017, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5975, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5933, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5891, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5850, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5810, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5770, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5730, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5690, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5651, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5612, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5574, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5536, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5499, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5461, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5425, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5388, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5352, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5316, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5281, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5246, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5211, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5176, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5142, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5108, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5075, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5042, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.5009, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4976, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4944, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4912, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4880, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4849, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4818, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4787, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4756, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4726, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4696, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4666, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4637, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4608, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4579, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4550, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4522, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4493, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4465, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4438, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4410, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4383, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4356, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4329, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4303, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4277, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4251, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4225, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4199, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4174, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4149, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4124, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4099, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4074, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4050, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4026, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.4002, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3978, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3955, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3932, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3908, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3886, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3863, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3840, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3818, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3796, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3774, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3752, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3730, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3709, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3687, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3666, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3645, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3625, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3604, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3584, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3563, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3543, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3523, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3503, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3484, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3464, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3445, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3426, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3407, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3388, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3369, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3350, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3332, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3314, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3295, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3277, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3259, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3242, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3224, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3207, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3189, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3172, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3155, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3138, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3121, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3104, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3088, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3071, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3055, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3039, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3023, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.3007, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2991, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2975, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2959, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2944, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2929, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2913, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2898, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2883, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2868, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2853, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2839, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2824, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2809, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2795, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2781, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2766, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2752, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2738, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2724, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2711, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2697, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2683, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2670, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2656, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2643, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2630, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2617, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2603, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2590, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2578, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2565, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2552, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2539, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2527, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2515, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2502, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2490, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2478, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2466, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2454, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2442, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2430, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2418, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2406, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2395, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2383, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2372, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2360, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2349, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2338, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2327, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2315, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2304, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2293, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2283, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2272, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2261, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2250, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2240, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2229, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2219, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2208, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2198, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2188, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2178, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2168, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2158, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2148, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2138, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2128, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2118, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2108, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2099, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2089, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2079, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2070, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2060, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2051, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2042, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2033, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2023, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2014, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.2005, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1996, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1987, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1978, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1969, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1960, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1952, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1943, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1934, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1926, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1917, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1909, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1900, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1892, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1884, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1875, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1867, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1859, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1851, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1843, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1835, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1827, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1819, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1811, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1803, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1795, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1787, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1780, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1772, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1764, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1757, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1749, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1742, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1734, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1727, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1720, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1712, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1705, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1698, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1691, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1684, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1677, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1669, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1662, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1655, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1649, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1642, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1635, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1628, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1621, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1614, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1608, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1601, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1594, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1588, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1581, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1575, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1568, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1562, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1555, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1549, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1543, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1536, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1530, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1524, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1518, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1512, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1506, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1499, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1493, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1487, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1481, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1475, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1469, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1464, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1458, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1452, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1446, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1440, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1435, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1429, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1423, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1418, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1412, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1406, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1401, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1395, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1390, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1384, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1379, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1374, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1368, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1363, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1357, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1352, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1347, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1342, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1336, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1331, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1326, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1321, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1316, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1311, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1306, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1301, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1296, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1291, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1286, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1281, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1276, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1271, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1266, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1262, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1257, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1252, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1247, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1243, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1238, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1233, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1229, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1224, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1219, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1215, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1210, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1206, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1201, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1197, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1192, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1188, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1183, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1179, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1175, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1170, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1166, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1162, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1157, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1153, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1149, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1145, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1140, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1136, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1132, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1128, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1124, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1120, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1116, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1112, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1108, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1104, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1100, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1096, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1092, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1088, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1084, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1080, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1076, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1072, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1068, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1064, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1061, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1057, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1053, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1049, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1046, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1042, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1038, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1034, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1031, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1027, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1024, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1020, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1016, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1013, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1009, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1006, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.1002, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0999, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0995, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0992, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0988, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0985, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0981, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0978, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0974, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0971, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0968, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0964, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0961, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0958, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0954, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0951, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0948, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0945, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0941, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0938, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0935, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0932, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0928, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0925, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0922, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0919, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0916, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0913, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0910, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0907, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0903, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0900, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0897, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0894, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0891, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0888, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0885, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0882, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0879, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0876, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0874, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0871, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0868, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0865, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0862, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0859, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0856, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0853, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0850, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0848, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0845, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0842, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0839, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0836, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0834, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0831, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0828, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0825, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0823, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0820, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0817, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0815, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0812, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0809, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0807, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0804, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0801, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0799, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0796, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0794, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0791, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0789, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0786, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0783, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0781, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0778, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0776, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0773, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0771, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0768, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0766, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0763, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0761, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0759, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0756, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0754, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0751, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0749, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0747, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0744, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0742, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0739, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0737, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0735, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0732, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0730, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0728, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0726, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0723, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0721, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0719, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0716, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0714, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0712, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0710, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0707, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0705, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0703, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0701, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0699, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0696, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0694, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0692, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0690, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0688, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0686, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0684, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0681, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0679, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0677, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0675, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0673, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0671, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0669, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0667, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0665, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0663, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0661, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0659, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0657, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0655, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0653, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0651, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0649, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0647, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0645, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0643, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0641, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0639, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0637, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0635, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0633, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0631, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0629, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0627, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0625, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0624, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0622, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0620, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0618, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0616, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0614, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0612, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0611, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0609, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0607, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0605, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0603, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0602, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0600, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0598, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0596, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0594, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0593, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0591, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0589, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0587, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0586, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0584, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0582, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0580, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0579, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0577, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0575, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0574, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0572, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0570, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0569, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0567, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0565, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0564, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0562, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0560, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0559, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0557, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0555, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0554, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0552, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0551, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0549, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0547, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0546, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0544, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0543, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0541, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0539, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0538, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0536, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0535, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0533, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0532, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0530, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0529, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0527, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0526, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0524, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0523, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0521, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0520, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0518, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0517, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0515, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0514, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0512, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0511, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0509, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0508, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0506, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0505, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0504, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0502, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0501, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0499, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0498, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0496, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0495, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0494, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0492, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0491, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0489, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0488, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0487, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0485, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0484, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0483, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0481, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0480, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0479, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0477, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0476, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0475, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0473, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0472, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0471, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0469, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0468, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0467, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0465, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0464, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0463, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0462, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0460, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0459, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0458, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0456, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0455, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0454, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0453, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0451, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0450, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0449, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0448, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0446, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0445, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0444, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0443, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0442, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0440, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0439, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0438, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0437, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0436, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0434, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0433, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0432, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0431, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0430, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0428, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0427, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0426, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0425, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0424, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0423, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0422, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0420, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0419, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0418, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0417, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0416, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0415, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0414, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0412, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0411, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0410, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0409, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0408, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0407, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0406, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0405, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0404, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0403, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0401, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0400, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0399, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0398, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0397, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0396, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0395, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0394, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0393, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0392, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0391, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0390, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0389, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0388, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0387, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0386, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0385, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0384, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0383, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0382, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0381, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0380, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0379, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0378, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0377, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0376, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0375, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0374, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0373, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0372, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0371, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0370, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0369, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0368, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0367, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0366, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0365, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0364, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0363, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0362, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0361, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0360, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0359, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0358, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0357, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0356, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0355, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0354, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0354, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0353, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0352, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0351, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0350, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0349, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0348, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0347, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0346, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0345, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0344, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0344, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0343, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0342, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0341, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0340, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0339, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0338, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0337, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0337, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0336, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0335, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0334, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0333, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0332, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0331, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0330, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0330, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0329, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0328, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0327, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0326, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0325, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0325, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0324, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0323, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0322, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0321, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0320, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0320, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0319, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0318, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0317, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0316, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0316, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0315, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0314, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0313, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0312, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0312, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0311, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0310, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0309, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0308, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0308, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0307, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0306, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0305, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0305, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0304, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0303, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0302, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0301, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0301, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0300, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0299, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0298, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0298, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0297, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0296, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0295, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0295, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0294, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0293, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0292, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0292, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0291, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0290, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0289, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0289, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0288, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0287, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0287, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0286, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0285, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0284, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0284, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0283, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0282, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0282, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0281, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0280, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0279, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0279, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0278, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0277, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0277, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0276, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0275, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0275, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0274, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0273, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0273, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0272, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0271, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0271, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0270, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0269, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0269, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0268, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0267, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0267, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0266, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0265, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0265, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0264, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0263, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0263, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0262, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0261, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0261, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0260, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0259, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0259, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0258, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0257, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0257, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0256, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0256, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0255, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0254, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0254, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0253, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0252, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0252, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0251, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0251, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0250, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0249, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0249, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(4.0248, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " ...]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MatrixFactorizer(4,4,1)\n",
    "data = torch.from_numpy(np.empty([4,4])) \n",
    "#test_random_tensor =  torch.from_numpy(np.empty([4,4])) \n",
    "train2(model,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
